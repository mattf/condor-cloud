<?xml version="1.0" encoding="iso-8859-1"?>
<!DOCTYPE html 
     PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
     "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
  <title>File: README</title>
  <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1" />
  <meta http-equiv="Content-Script-Type" content="text/javascript" />
  <link rel="stylesheet" href=".././rdoc-style.css" type="text/css" media="screen" />
  <script type="text/javascript">
  // <![CDATA[

  function popupCode( url ) {
    window.open(url, "Code", "resizable=yes,scrollbars=yes,toolbar=no,status=no,height=150,width=400")
  }

  function toggleCode( id ) {
    if ( document.getElementById )
      elem = document.getElementById( id );
    else if ( document.all )
      elem = eval( "document.all." + id );
    else
      return false;

    elemStyle = elem.style;
    
    if ( elemStyle.display != "block" ) {
      elemStyle.display = "block"
    } else {
      elemStyle.display = "none"
    }

    return true;
  }
  
  // Make codeblocks hidden by default
  document.writeln( "<style type=\"text/css\">div.method-source-code { display: none }</style>" )
  
  // ]]>
  </script>

</head>
<body>



  <div id="fileHeader">
    <h1>README</h1>
    <table class="header-table">
    <tr class="top-aligned-row">
      <td><strong>Path:</strong></td>
      <td>README
      </td>
    </tr>
    <tr class="top-aligned-row">
      <td><strong>Last Update:</strong></td>
      <td>Thu Mar 24 04:01:49 +0100 2011</td>
    </tr>
    </table>
  </div>
  <!-- banner header -->

  <div id="bodyContent">



  <div id="contextContent">

    <div id="description">
      <p>
Condor Cloud <a href="../classes/Deltacloud.html">Deltacloud</a> API driver
~~~~~~~~~~~~
</p>
<p>
Requirements:
</p>
<ul>
<li>Condor installed using steps from &#8216;Condor Cloud&#8217; section

</li>
<li>Commands in shell path: condor_q, condor_submit and condor_rm

</li>
<li>VM images in /home/cloud directory (this can be overwritten using
IMAGE_STORAGE environment variable

</li>
</ul>
<p>
Instalation:
</p>
<ol>
<li>git clone git://git.apache.org/deltacloud.git core

</li>
<li>Copy deltacloud-condor-driver/* into
core/server/lib/deltacloud/drivers/condor

</li>
<li>Edit core/server/config/drivers.yaml and add following lines to enable
Condor driver:

</li>
<li>Copy config/addresses.xml into core/server/config/

</li>
<li>Start <a href="../classes/Deltacloud.html">Deltacloud</a> API using:
./bin/deltacloudd -i condor

</li>
</ol>
<p>
Testing &amp;&amp; Developing:
</p>
<p>
Test::Unit files are located in &#8216;test&#8217; directory, but to
execute them you need to do some modifications in executor.rb. First you
need to change a path for base_model, because it&#8216;s set to <a
href="../classes/Deltacloud.html">Deltacloud</a> API default dir. Then you
can execute test suite using: &#8216;rake test&#8216;
</p>
<p>
Condor Cloud ~~~~~~~~~~~~
</p>
<p>
This is a set of Condor configuration settings and scripts for execute
nodes of an IaaS cloud, and basic scripts for interaction.
</p>
<p>
It supports launching three types of stateless instances; small (512MB RAM,
1 CPU), medium (1GB RAM, 2 CPU), and large (2GB RAM, 4 CPU). A users
produces a QEMU disk image including a kernel and initrd, uploads it with
add_image.sh, launches it as many times as desired with launch_image.sh,
monitors with list_instances.sh and terminates by either powering the
instance off from within or running terminate_instances.sh.
</p>
<p>
All instances get a VNC console and a network interface with a dynamic IP.
</p>
<p>
All instances are stateless, meaning they are given a fresh disk on each
launch.
</p>
<p>
NOTE: It may be necessary to <b>disable</b> SELinux
</p>
<p>
functions -
</p>
<p>
Set of utility functions used by the shell scripts below. It must be
installed in /opt/condor-cloud so it can be source&#8216;d.
</p>
<p>
cloud.config -
</p>
<p>
Condor configuration for an execute node. Set this as LOCAL_CONFIG_FILE
inside ~condor/condor_config.local, or drop it in a LOCAL_CONFIG_DIR. It
does:
</p>
<pre>
 1) Setup of the Virtual Machine (VM) Universe to accept KVM jobs, and
    use a custom libvirt XML configuration script
    (libvirt_cloud_script.sh)
 2) Enabling of a single partitionable slot for VM packing, with 4x
    resources (over provisioning)
 3) Setup of a PREPARE job hook to retrieve the VM's base image and
    produce a qcow2 image from it
 4) Setup of a STARTD_CRON (aka Hawkeye) script to advertise base
    images already available on the execute node (cached_images.sh)
</pre>
<p>
cached_images.sh -
</p>
<p>
A Hawkeye script to advertise images already available on the node in a
CACHED_IMAGES attribute. Useful for RANK expressions so VMs prefer to run
on nodes that already hold their base image.
</p>
<p>
Install as $(condor_config_val STARTD_CRON_CACHED_IMAGES_EXECUTABLE) with
permissions set to a=rx.
</p>
<p>
libvirt_cloud_script.sh -
</p>
<p>
The libvirt XML configuration script. It accepts a job ad on stdin and
produces the XML to create a domain on stdout. The XML is provided by the
job&#8216;s VM_XML attribute, set from launch_image.sh. The VM_XML is
processed to set the {NAME}, which is decided by the vm-gahp.
</p>
<p>
Install as $(condor_config_val LIBVIRT_XML_SCRIPT) with permissions set to
a=rx.
</p>
<p>
cloud_prepare_hook.sh -
</p>
<p>
The job hook PREPARE script. It accepts a job ad on stdin, creates the
job&#8216;s disk image (qcow2), downloading the base image if needed, and
produces attributes updates to the job ad (one per line). It uses the Cmd
(executable) attribute on the job ad as the name of the job&#8216;s base
image. It processes the job ad&#8216;s VM_XML attribute and sets the {DISK}
portion.
</p>
<p>
Install as $(condor_config_val CLOUD_HOOK_PREPARE_JOB) with permissions set
to a=rx.
</p>
<p>
cloud_exit_hook.sh -
</p>
<p>
The job hook EXIT script. It is invoked when the VM exits. It takes a job
ad on stdin, finds the disk image, and removes it. It currently does not
work because libvirtd changes ownership of the disk image to root.root when
the VM exits, instead of back to condor.condor.
</p>
<p>
Install as $(condor_config_val CLOUD_HOOK_JOB_EXIT) with permissions set to
a=rx.
</p>
<p>
add_image.sh - list_images.sh - launch_image.sh - list_instances.sh -
terminate_instances.sh -
</p>
<p>
These scripts do what their names suggest.
</p>
<p>
ToDo:
</p>
<pre>
 o Report VNC console port into job ad, possibly with job hook STATUS
   script, or PREPARE hook since it is invoked by the starter
 o Expand to 64 and 32 bit images, currently 32 bit
 o Properly label disk images to play nicely with SELinux policy
 o Allow for watchdog processes within VMs
 o Allow for fixed IPs - MAC allocator and DHCP server, or just static IP?
 o Work out disk sizes and necessary storage in cache based on
   possible number of concurrent instances
 o Allow for stateful instances, possibly using concurrency limits
 o Provide a security model for images
 o Provide a QMF or dcloud interface
 o Improve vm-gahp error reporting in Condor
 o ...
</pre>
<p>
Setup br0:
</p>
<p>
cat &gt; /etc/sysconfig/network-scripts/ifcfg-br0 &lt;&lt; EOF DEVICE=br0
TYPE=Bridge BOOTPROTO=dhcp ONBOOT=yes DELAY=0 EOF
</p>
<p>
cat &gt;&gt; /etc/sysconfig/network-scripts/ifcfg-eth0 &lt;&lt; EOF
BRIDGE=br0 EOF
</p>
<p>
cat &gt;&gt; /etc/sysctl.conf &lt;&lt; EOF
</p>
<p>
# Disable iptables on bridged, primarily for br0
net.bridge.bridge-nf-call-ip6tables = 0 net.bridge.bridge-nf-call-iptables
= 0 net.bridge.bridge-nf-call-arptables = 0 EOF
</p>
<p>
sysctl -p /etc/sysctl.conf
</p>
<p>
service network restart
</p>
<p>
service libvirtd reload
</p>

    </div>


   </div>


  </div>


    <!-- if includes -->

    <div id="section">





      


    <!-- if method_list -->


  </div>


<div id="validator-badges">
  <p><small><a href="http://validator.w3.org/check/referer">[Validate]</a></small></p>
</div>

</body>
</html>